{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Neural Network from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What is a Neural Network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks are a subset of machine learning that aims to simulate how the human brain works. They take data and essentially train themselves to gain insight out of the data before being able to predict the output for data inputted into the model in the future. \n",
    "\n",
    "Artificial Neural Networks are essentially a relationship between inputs and outputs where each connection has an importance (weight). This is exactly how our brain works when making decisions - when faced with 'data' our brain will prioritise the most important features of that data to make a decision on what that data is or what decisions to make next. \n",
    "\n",
    "Neural Networks can be used for a multitude of things from classification to speech recognition to computer vision to Natural Language Processing. A neural network has an input layer, a hidden layer, and an output layer, and a perceptron is one without a hidden layer. The latter tend to be used for much simpler decision making. \n",
    "\n",
    "A neural network has two steps essentially:\n",
    "\n",
    "* Feedforward propagation which describes a forward movement of information using random weights, these random weights are then improved using backpropagation. \n",
    "* Backpropagation is essentially the calculation of an error between a predicted output and a target which allows the model to update the weight values afterwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight given for each input is essentially dictating how important that piece of information is to making the final prediction. Therefore ensuring the weights are as correct as possible is vital to a successful model. Backpropagation allows us to start with random weights and then adjust these weights once we can see the first round of predictions. This process of going back and altering the weight is called backpropagation, and we repeat this until the error is minimised.\n",
    "\n",
    "Therefore an Artificial Neural Network can be summarised as a 'cycle' of some sorts. The model takes in an input and randomly assigns weight (we can assign weights manually which will create bias), the output is created, the error is calculated, then the weights are updated and the process will repeat itself. This continues until the error is acceptable and only then will the model be ready for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Derivation of the Backpropagation Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need three things to train a neural network using gradient descent. Firstly, we need a dataset with input and output pairs of size N and this is called X. Then, we need a feedforward neural network with parameters. The parameter that is important is the weight between node j in layer k and node i in layer k-1, and the bias for node i in layer k. Thirdly, we need an error function that is able to define the error between the needed y value and the outputted y value from the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $θ^{t}$ is considered the collective parameters of the neural network for iteration $t$ then gradient descent will update these parameters using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "θ^{t+1} = θ^{t} - α\\frac{\\partial E(X,θ^{t})}{\\partial θ} \\tag{1}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $X$ is the dataset with $x,y$ value pairs, $α$ is the learning rate, and $E(X,θ)$ is the error function with respect to weights $w_{ij}^{k}$ and biases $b_{i}^{k}$. This says that as error tends towards zero the weights between iterations will tend towards being equal meaning we have reached an 'optimum weight'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below uses mean of square errors which is one of many error functions that can be used depending on need. This tends to be the standard error function as it has always been associated with errors. Here we multiply by $\\frac {1}{2}$ to eliminate the 2 that comes down from the square when differentiating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(X,θ) = \\frac{1}{2N}\\sum_{i=1}^{N}(\\hat{y_{i}} - y_{i})^{2} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $y_{i}$ is the target value in the input-output pair, and $\\hat{y_{i}}$ is the computed output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layers in a neural network don't have a defined output so errors will be computed using parameters of previous and following layers. Backpropagation aims to simplify the mathematics of gradient descent between layers. Taking the error function above, backpropagation aims to minimise this function with respect to the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E(X,θ)}{\\partial w_{ij}^{k}} = \\frac{1}{N} \\sum_{d=1}^{N} \\frac{\\partial (\\frac{1}{2} (\\hat{y_{d}} - y_{d})^{2})}{\\partial w_{ij}^{k}} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider $E_{d}$ = $\\frac{1}{2} (\\hat{y_{d}} - y_{d})^{2}$ then we can write the above as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E(X,θ)}{\\partial w_{ij}^{k}} =\\frac{1}{N} \\sum_{d=1}^{N} \\frac{\\partial E_{d}}{\\partial w_{ij}^{k}} \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to continue with the derivation of the backpropagation algorithm, we need to calculate the activation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{i}^{k} = b_{i}^{k} + \\sum_{j=1}^{r_{k-1}} w_{ji}^{k}o_{j}^{k-1} \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $w_{ij}^{k}$ is the weight for node $j$ in layer $l_{k}$ for incoming node $i$, $b_{i}^{k}$ is the bias for node $i$ in layer $l_{k}$, $a_{i}^{k}$ is the product sum plus bias for node $i$ in layer $l_{k}$, $o_{i}^{k}$ is the output for note $i$ in layer $l_{k}$, and $r_{k}$ is the number of nodes in layer $l_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplification purposes, we can consider $b_{i}^{k}$ an initial 'weight' for stage $j = 0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{i}^{k} = \\sum_{j=0}^{r_{k-1}} w_{ji}^{k}o_{j}^{k-1} \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent updates weights and biases - to simplify mathematics we combined these two in the stage above and considered bias as an initial weight for stage j = 0 leaving us with only one function for weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of these, we can start to work out the error function derivative. We use the chain rule to get the error function's partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial E}{\\partial w_{ij}^{k}} = \\frac {\\partial E}{\\partial a_{j}^{k}} \\frac {\\partial a_{j}^{k}}{\\partial w_{ij}^{k}} \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $a_{j}^{k}$ is the activation of node j in layer k before its passed to the nonlinear activation function to generate output. Neural Networks tend to use non-learn activation functions as they make it easier to adapt data and differentiate between outcomes. For a two-class classifier, a sigmoid activation function is used as the output is always between 0 and 1 for input (-∞,∞). Other non-linear activation functions can include Tanh which will map values to between -1 and 1. More complex non-linear activation functions include ReLU used in computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above expression, $\\frac {\\partial E}{\\partial a_{j}^{k}}$ can be called the **error** $δ_{j}^{k}$. The reason for this is where backpropagation gets its name. For a single output neural network, backpropagation defines the value $δ_{1}^{m}$ where $m$ is the final layer. So for this neural network we can express the error function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = \\frac {1}{2} (\\hat {y} - y)^{2} = \\frac {1}{2} (g_{o}(a_{1}^{m}) - y)^{2} \\tag{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $g_{o}(x)$ is the activation function for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore differentiating the above we can get $δ_{1}^{m}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "δ_{1}^{m} = (g_{0}(a_{1}^{m}) - y)g_{o}^{'}(a_{a}^{m}) = (\\hat {y} - y)g_{o}^{'}(a_{1}^{m}) \\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In equation $(6)$ and $(7)$ we can express $\\frac {\\partial a_{j}^{k}}{\\partial w_{ij}^{k}}$ as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial a_{j}^{k}}{\\partial w_{ij}^{k}} = \\frac {\\partial}{\\partial w_{ij}^{k}} \\left(\\sum _{l=0}^{r_{k-1}} w_{ij}^{k} o_{l}^{k-1}\\right) = o_{i}^{k-1} \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting $(9)$ and $(10)$ together we get the partial derivative of the error function $E$ with respect to a weight in the final layer $w_{i1}^{m}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial E}{\\partial w_{i1}^{m}} = δ_{1}^{m}o_{i}^{m-1} = (\\hat {y} - y)g_{o}^{'}(a_{1}^{m}) o_{i}^{m-1} \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is good for the final layer of a neural network where we have an output, we now need to calculate the partial derivatives of hidden layers. We can do so using the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "δ_{j}^{k} = \\frac {\\partial E}{\\partial a_{j}^{k}} = \\sum_{l=1}^{r^{k+1}} \\frac {\\partial E}{\\partial a_{l}^{k+1}} \\frac {\\partial a_{l}^{k+1}}{\\partial a_{j}^{k}} \\tag{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $1 \\leq k < m$, and $l$ ranges from 1 to the number of nodes in the next layer $r^{k+1}$, $l$ doesn't start from 0 since the bias for layer 0 does not fluctuate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simplify $(12)$ by substituting for $δ_{l}^{k+1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "δ_{j}^{k} = \\sum_{l=1}^{r^{k+1}} δ_{l}^{k+1} \\frac {\\partial a_{l}^{k+1}}{\\partial a_{j}^{k}} \\tag{13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reminding ourselves of $(6)$ we can differentiate $a_{l}^{k+1}$ with respect to $a_{j}^{k}$ to get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial a_{l}^{k+1}}{\\partial a_{j}^{k}} = w_{jl}^{k+1}g^{'}(a_{j}^{k}) \\tag{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the overall partial derivative of the error dunction $E$ with respect to a weight in the hidden layers $w_{ij}^{k}$ for $1 \\leq k < m$ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial E}{\\partial w_{ij}^{k}} = g^{'}(a_{j}^{k}) o_{i}^{k-1} \\sum _{l=1}^{r^{k+1}} δ_{l}^{k+1} w_{jl}^{k+1} \\tag{15}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error $δ_{j}^{k}$ at layer $k$ is dependent on the errors $δ_{k}^{k+1}$ at the next layer. Once error in final layer between $\\hat {y} = g_{o}(a_{1}^{m})$ and target $y$ is computed we can work backwards, carrying out a sum of that error multiplied by the weight, and multiplying this by $g^{'}(a_{j}^{k})$ until input layer is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every iteration of neural network training, the forward phase will calculate the outputs and relevant activation and outputs for each layer will be remembered for the backwards phase. The backwards phase will use these activations and outputs to calculate the partial derivatives of the error function with respect to weights and gradient descent is used to update these weights. These iterations are repeated until a local minimum is found or another criterion is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sigmoid Activation Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classic neural network, the activation function for hidden nodes is sigmoidal and the activation for the output layer is the identity function. The differential of the sigmoid function is the main factor to why it is so widely used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g^{'}(x) = \\frac {\\partial {σ(x)}}{\\partial {x}} = σ(x)(1-σ(x)) \\tag {16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $σ(x)$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, since the output function uses the identity function, the derivative of this is simply $g_{o}^{'}(x) = 1$. Due to this simplicity, activation values and output values can be forgotten leading to less memory usage by the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the error term for the final layer is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "δ_{1}^{m} = \\hat {y_{d}} - y_{d} \\tag {17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the error term for the hidden layers is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "δ_{j}^{k} = o_{j}^{k}(1 - o_{j}^{k}) \\sum _{l=1}^{r^{k+1}} w_{jl}^{k+1}δ_{l}^{k+1} \\tag {18}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries, classes, and functions \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Classes.Layer import Layer\n",
    "from Classes.NeuralNetwork import NeuralNetwork\n",
    "from Functions.Sigmoid import sigmoid\n",
    "from Functions.Loss import meanSquareError\n",
    "from Functions.deEnoder import deEncoder\n",
    "from Functions.toList import toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Data/mnist_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('Data/mnist_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = ['label'])\n",
    "y_train = train['label']\n",
    "X_test = test.drop(columns = ['label'])\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = []\n",
    "\n",
    "for i in y_train :\n",
    "    if i == 0 :\n",
    "        y_train_enc.append([1,0,0,0,0,0,0,0,0,0])\n",
    "    elif i == 1 :\n",
    "        y_train_enc.append([0,1,0,0,0,0,0,0,0,0])\n",
    "    elif i == 2 :\n",
    "        y_train_enc.append([0,0,1,0,0,0,0,0,0,0])\n",
    "    elif i == 3 :\n",
    "        y_train_enc.append([0,0,0,1,0,0,0,0,0,0])\n",
    "    elif i == 4 :\n",
    "        y_train_enc.append([0,0,0,0,1,0,0,0,0,0])\n",
    "    elif i == 5 :\n",
    "        y_train_enc.append([0,0,0,0,0,1,0,0,0,0])\n",
    "    elif i == 6 :\n",
    "        y_train_enc.append([0,0,0,0,0,0,1,0,0,0])\n",
    "    elif i == 7 :\n",
    "        y_train_enc.append([0,0,0,0,0,0,0,1,0,0])\n",
    "    elif i == 8 :\n",
    "        y_train_enc.append([0,0,0,0,0,0,0,0,1,0])\n",
    "    elif i == 9 :\n",
    "        y_train_enc.append([0,0,0,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_enc = []\n",
    "\n",
    "for i in y_test :\n",
    "    if i == 0 :\n",
    "        y_test_enc.append([1,0,0,0,0,0,0,0,0,0])\n",
    "    elif i == 1 :\n",
    "        y_test_enc.append([0,1,0,0,0,0,0,0,0,0])\n",
    "    elif i == 2 :\n",
    "        y_test_enc.append([0,0,1,0,0,0,0,0,0,0])\n",
    "    elif i == 3 :\n",
    "        y_test_enc.append([0,0,0,1,0,0,0,0,0,0])\n",
    "    elif i == 4 :\n",
    "        y_test_enc.append([0,0,0,0,1,0,0,0,0,0])\n",
    "    elif i == 5 :\n",
    "        y_test_enc.append([0,0,0,0,0,1,0,0,0,0])\n",
    "    elif i == 6 :\n",
    "        y_test_enc.append([0,0,0,0,0,0,1,0,0,0])\n",
    "    elif i == 7 :\n",
    "        y_test_enc.append([0,0,0,0,0,0,0,1,0,0])\n",
    "    elif i == 8 :\n",
    "        y_test_enc.append([0,0,0,0,0,0,0,0,1,0])\n",
    "    elif i == 9 :\n",
    "        y_test_enc.append([0,0,0,0,0,0,0,0,0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = np.array(y_train_enc)\n",
    "y_test_enc = np.array(y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating & Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Error = 0.013277\n",
      "\n",
      "Start of epoch 2\n",
      "Error = 0.007431\n",
      "\n",
      "Start of epoch 3\n",
      "Error = 0.007013\n",
      "\n",
      "Start of epoch 4\n",
      "Error = 0.007455\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "\n",
    "network = NeuralNetwork()\n",
    "network.add(Layer(28*28, 100, sigmoid))\n",
    "network.add(Layer(100, 50, sigmoid))\n",
    "network.add(Layer(50, 10, sigmoid))\n",
    "\n",
    "# train on 1000 samples\n",
    "network.setLossFunction(meanSquareError)\n",
    "network.fit(X_train, y_train_enc, epochs = 4, learning_rate = 0.1) \n",
    "\n",
    "output = network.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_arr = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.99396414e-03, 7.80754360e-05, 9.66844781e-04, 5.94526984e-03,\n",
       "        7.57109907e-07, 1.67478064e-04, 1.23265766e-07, 9.95101965e-01,\n",
       "        5.43093662e-05, 2.02040065e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting output to Python List\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in range(len(output_arr)) :\n",
    "    output_list.append(output_arr[i][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting output into integers - maximum value becomes 1 the rest becomes 0.\n",
    "\n",
    "output_int = []\n",
    "\n",
    "for i in output_list :\n",
    "    x = []\n",
    "    for j in i :\n",
    "        if j == max(i) :\n",
    "            x.append(1)\n",
    "        else :\n",
    "            x.append(0)\n",
    "    output_int.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_denc = deEncoder(output_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 2, 1, 0, 4, 1, 4, 9, 6, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_denc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
